{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import (\n",
    "        accuracy_score, precision_score,\n",
    "        recall_score, f1_score\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalysisFramework:\n",
    "    def __init__(self, dataset_name):\n",
    "        self.dataset_name = dataset_name\n",
    "        self.dataset = load_dataset(dataset_name)\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"\n",
    "        Comprehensive text preprocessing\n",
    "        \"\"\"\n",
    "        import re\n",
    "        import nltk\n",
    "        from nltk.corpus import stopwords\n",
    "        from nltk.tokenize import word_tokenize\n",
    "\n",
    "        # Download necessary NLTK resources\n",
    "        nltk.download('punkt', quiet=True)\n",
    "        nltk.download('stopwords', quiet=True)\n",
    "\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "\n",
    "        # Remove special characters and numbers\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "        # Tokenization\n",
    "        tokens = word_tokenize(text)\n",
    "\n",
    "        # Remove stopwords\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    def feature_extraction(self):\n",
    "        from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "        from gensim.models import Word2Vec\n",
    "\n",
    "        # Convert to pandas\n",
    "        train_df = self.dataset['train'].to_pandas()\n",
    "        test_df = self.dataset['test'].to_pandas()\n",
    "\n",
    "        # Standardize column names\n",
    "        if 'text' not in train_df.columns:\n",
    "            if 'content' in train_df.columns:\n",
    "                train_df.rename(columns={'content': 'text'}, inplace=True)\n",
    "                test_df.rename(columns={'content': 'text'}, inplace=True)\n",
    "            elif 'sentence' in train_df.columns:\n",
    "                train_df.rename(columns={'sentence': 'text'}, inplace=True)\n",
    "                test_df.rename(columns={'sentence': 'text'}, inplace=True)\n",
    "\n",
    "        # Preprocessing\n",
    "        train_df['cleaned_text'] = train_df['text'].apply(self.preprocess_text)\n",
    "        test_df['cleaned_text'] = test_df['text'].apply(self.preprocess_text)\n",
    "\n",
    "        # Bag of Words\n",
    "        bow_vectorizer = CountVectorizer(max_features=5000)\n",
    "        bow_train = bow_vectorizer.fit_transform(train_df['cleaned_text'])\n",
    "        bow_test = bow_vectorizer.transform(test_df['cleaned_text'])\n",
    "\n",
    "        # TF-IDF\n",
    "        tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "        tfidf_train = tfidf_vectorizer.fit_transform(train_df['cleaned_text'])\n",
    "        tfidf_test = tfidf_vectorizer.transform(test_df['cleaned_text'])\n",
    "\n",
    "        # Word2Vec\n",
    "        word2vec = self._word2vec_features(train_df['cleaned_text'], test_df['cleaned_text'])\n",
    "\n",
    "        features = {\n",
    "            'Bag of Words': {\n",
    "                'train': bow_train,\n",
    "                'test': bow_test\n",
    "            },\n",
    "            'TF-IDF': {\n",
    "                'train': tfidf_train,\n",
    "                'test': tfidf_test\n",
    "            },\n",
    "            'Word2Vec': word2vec\n",
    "        }\n",
    "\n",
    "        return {\n",
    "            'features': features,\n",
    "            'labels': {\n",
    "                'train': train_df['label'],\n",
    "                'test': test_df['label']\n",
    "            },\n",
    "            'original_texts': {\n",
    "                'train': train_df['text'],\n",
    "                'test': test_df['text']\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def _word2vec_features(self, train_texts, test_texts):\n",
    "        \"\"\"\n",
    "        Word2Vec feature extraction\n",
    "        \"\"\"\n",
    "        import numpy as np\n",
    "        from gensim.models import Word2Vec\n",
    "\n",
    "        # Tokenize texts\n",
    "        train_tokens = [text.split() for text in train_texts]\n",
    "        test_tokens = [text.split() for text in test_texts]\n",
    "\n",
    "        # Train Word2Vec model\n",
    "        w2v_model = Word2Vec(sentences=train_tokens, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "        # Create document vectors\n",
    "        def document_vector(tokens):\n",
    "            vectors = [w2v_model.wv[word] for word in tokens if word in w2v_model.wv]\n",
    "            return np.mean(vectors, axis=0) if vectors else np.zeros(100)\n",
    "\n",
    "        train_vectors = np.array([document_vector(tokens) for tokens in train_tokens])\n",
    "        test_vectors = np.array([document_vector(tokens) for tokens in test_tokens])\n",
    "\n",
    "        return {\n",
    "            'train': train_vectors,\n",
    "            'test': test_vectors\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Model Evaluation and Comparison\n",
    "class SentimentModelComparison:\n",
    "    def __init__(self, features, labels, original_texts):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        self.original_texts = original_texts\n",
    "    \n",
    "    \n",
    "\n",
    "    def traditional_ml_models(self):\n",
    "        \"\"\"\n",
    "        Evaluate traditional machine learning models across feature types.\n",
    "        Skips models incompatible with certain feature formats (e.g. MultinomialNB with Word2Vec).\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        # Define models to evaluate\n",
    "        models = {\n",
    "            'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "            'Decision Tree': DecisionTreeClassifier(),\n",
    "            'Naive Bayes': MultinomialNB()\n",
    "        }\n",
    "\n",
    "        results = {}\n",
    "\n",
    "        # Loop through each feature extraction method\n",
    "        for feature_name, feature_data in self.features.items():\n",
    "            X_train = feature_data['train']\n",
    "            X_test = feature_data['test']\n",
    "            y_train = self.labels['train']\n",
    "            y_test = self.labels['test']\n",
    "\n",
    "            feature_results = {}\n",
    "\n",
    "            for model_name, model in models.items():\n",
    "                # Skip incompatible combinations\n",
    "                if feature_name == 'Word2Vec' and model_name == 'Naive Bayes':\n",
    "                    print(f\"⚠️ Skipping {model_name} on {feature_name} (contains negative values).\")\n",
    "                    continue\n",
    "\n",
    "                # Train and evaluate model\n",
    "                model.fit(X_train, y_train)\n",
    "                y_pred = model.predict(X_test)\n",
    "\n",
    "                feature_results[model_name] = {\n",
    "                    'Accuracy': accuracy_score(y_test, y_pred),\n",
    "                    'Precision': precision_score(y_test, y_pred, average='weighted', zero_division=0),\n",
    "                    'Recall': recall_score(y_test, y_pred, average='weighted', zero_division=0),\n",
    "                    'F1 Score': f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "                }\n",
    "\n",
    "            results[feature_name] = feature_results\n",
    "\n",
    "        return results\n",
    "\n",
    "    \n",
    "    def vader_sentiment_analysis(self):\n",
    "        \"\"\"\n",
    "        Evaluate VADER sentiment analysis\n",
    "        \"\"\"\n",
    "        from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "        from sklearn.metrics import accuracy_score\n",
    "        \n",
    "        # Initialize VADER sentiment analyzer\n",
    "        sid = SentimentIntensityAnalyzer()\n",
    "        \n",
    "        # Predict sentiment using VADER\n",
    "        def vader_predict(text):\n",
    "            # Get polarity scores\n",
    "            scores = sid.polarity_scores(text)\n",
    "            \n",
    "            # Classify based on compound score\n",
    "            # Positive: compound score > 0\n",
    "            # Negative: compound score < 0\n",
    "            return 1 if scores['compound'] > 0 else 0\n",
    "        \n",
    "        # Apply VADER to test texts\n",
    "        y_pred = [vader_predict(text) for text in self.original_texts['test']]\n",
    "        y_true = self.labels['test']\n",
    "        \n",
    "        # Compute metrics\n",
    "        return {\n",
    "            'VADER': {\n",
    "                'Accuracy': accuracy_score(y_true, y_pred),\n",
    "                # Note: Other metrics might be challenging with VADER's simple classification\n",
    "            }\n",
    "        }\n",
    "    \n",
    "   \n",
    "   \n",
    "\n",
    "\n",
    "    def visualize_results(self, traditional_results, transformer_results, vader_results):\n",
    "        \"\"\"\n",
    "        Visualize model performance\n",
    "        \"\"\"\n",
    "        # Prepare data for visualization\n",
    "        model_performances = {}\n",
    "        \n",
    "        # Process traditional ML results\n",
    "        for feature, models in traditional_results.items():\n",
    "            for model, metrics in models.items():\n",
    "                key = f\"{model} ({feature})\"\n",
    "                model_performances[key] = metrics['Accuracy']\n",
    "        \n",
    "        # Process transformer results\n",
    "        for model, metrics in transformer_results.items():\n",
    "            model_performances[model] = metrics['Accuracy']\n",
    "        \n",
    "        # Process VADER results\n",
    "        for model, metrics in vader_results.items():\n",
    "            model_performances[model] = metrics['Accuracy']\n",
    "        \n",
    "        # Create bar plot\n",
    "        plt.figure(figsize=(14, 7))\n",
    "        bars = plt.bar(model_performances.keys(), model_performances.values())\n",
    "        plt.title('Sentiment Analysis Model Performance Comparison')\n",
    "        plt.xlabel('Models')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Add value labels on top of each bar\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                     f'{height:.4f}',\n",
    "                     ha='center', va='bottom')\n",
    "        \n",
    "        plt.savefig('model_performance.png')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing imdb dataset:\n",
      "⚠️ Skipping Naive Bayes on Word2Vec (contains negative values).\n",
      "\n",
      "Traditional ML Models Results:\n",
      "\n",
      "Bag of Words Feature Extraction:\n",
      "Logistic Regression:\n",
      "  Accuracy: 0.8483\n",
      "  Precision: 0.8484\n",
      "  Recall: 0.8483\n",
      "  F1 Score: 0.8483\n",
      "Decision Tree:\n",
      "  Accuracy: 0.7103\n",
      "  Precision: 0.7103\n",
      "  Recall: 0.7103\n",
      "  F1 Score: 0.7103\n",
      "Naive Bayes:\n",
      "  Accuracy: 0.8381\n",
      "  Precision: 0.8389\n",
      "  Recall: 0.8381\n",
      "  F1 Score: 0.8380\n",
      "\n",
      "TF-IDF Feature Extraction:\n",
      "Logistic Regression:\n",
      "  Accuracy: 0.8800\n",
      "  Precision: 0.8801\n",
      "  Recall: 0.8800\n",
      "  F1 Score: 0.8800\n",
      "Decision Tree:\n",
      "  Accuracy: 0.7079\n",
      "  Precision: 0.7081\n",
      "  Recall: 0.7079\n",
      "  F1 Score: 0.7078\n",
      "Naive Bayes:\n",
      "  Accuracy: 0.8424\n",
      "  Precision: 0.8428\n",
      "  Recall: 0.8424\n",
      "  F1 Score: 0.8424\n",
      "\n",
      "Word2Vec Feature Extraction:\n",
      "Logistic Regression:\n",
      "  Accuracy: 0.8047\n",
      "  Precision: 0.8048\n",
      "  Recall: 0.8047\n",
      "  F1 Score: 0.8047\n",
      "Decision Tree:\n",
      "  Accuracy: 0.6626\n",
      "  Precision: 0.6627\n",
      "  Recall: 0.6626\n",
      "  F1 Score: 0.6626\n",
      "\n",
      "Analyzing yelp_polarity dataset:\n",
      "⚠️ Skipping Naive Bayes on Word2Vec (contains negative values).\n",
      "\n",
      "Traditional ML Models Results:\n",
      "\n",
      "Bag of Words Feature Extraction:\n",
      "Logistic Regression:\n",
      "  Accuracy: 0.9246\n",
      "  Precision: 0.9246\n",
      "  Recall: 0.9246\n",
      "  F1 Score: 0.9245\n",
      "Decision Tree:\n",
      "  Accuracy: 0.7907\n",
      "  Precision: 0.7907\n",
      "  Recall: 0.7907\n",
      "  F1 Score: 0.7907\n",
      "Naive Bayes:\n",
      "  Accuracy: 0.8622\n",
      "  Precision: 0.8634\n",
      "  Recall: 0.8622\n",
      "  F1 Score: 0.8621\n",
      "\n",
      "TF-IDF Feature Extraction:\n",
      "Logistic Regression:\n",
      "  Accuracy: 0.9268\n",
      "  Precision: 0.9268\n",
      "  Recall: 0.9268\n",
      "  F1 Score: 0.9268\n",
      "Decision Tree:\n",
      "  Accuracy: 0.7926\n",
      "  Precision: 0.7926\n",
      "  Recall: 0.7926\n",
      "  F1 Score: 0.7926\n",
      "Naive Bayes:\n",
      "  Accuracy: 0.8752\n",
      "  Precision: 0.8753\n",
      "  Recall: 0.8752\n",
      "  F1 Score: 0.8752\n",
      "\n",
      "Word2Vec Feature Extraction:\n",
      "Logistic Regression:\n",
      "  Accuracy: 0.9053\n",
      "  Precision: 0.9053\n",
      "  Recall: 0.9053\n",
      "  F1 Score: 0.9053\n",
      "Decision Tree:\n",
      "  Accuracy: 0.7994\n",
      "  Precision: 0.7994\n",
      "  Recall: 0.7994\n",
      "  F1 Score: 0.7994\n",
      "\n",
      "Analyzing amazon_polarity dataset:\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'text'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 63\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[38;5;66;03m# print(\"\\nTransformer Models Results:\")\u001b[39;00m\n\u001b[1;32m     51\u001b[0m         \u001b[38;5;66;03m# for model, metrics in transformer_results.items():\u001b[39;00m\n\u001b[1;32m     52\u001b[0m         \u001b[38;5;66;03m#     print(f\"{model}:\")\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[38;5;66;03m#     for metric, value in metrics.items():\u001b[39;00m\n\u001b[1;32m     60\u001b[0m         \u001b[38;5;66;03m#             print(f\"  {metric}: {value:.4f}\")\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[23], line 13\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Preprocessing and Feature Extraction\u001b[39;00m\n\u001b[1;32m     12\u001b[0m analysis_framework \u001b[38;5;241m=\u001b[39m SentimentAnalysisFramework(dataset_name)\n\u001b[0;32m---> 13\u001b[0m extracted_features \u001b[38;5;241m=\u001b[39m analysis_framework\u001b[38;5;241m.\u001b[39mfeature_extraction()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Model Comparison\u001b[39;00m\n\u001b[1;32m     16\u001b[0m model_comparison \u001b[38;5;241m=\u001b[39m SentimentModelComparison(\n\u001b[1;32m     17\u001b[0m     extracted_features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m     18\u001b[0m     extracted_features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     19\u001b[0m     extracted_features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moriginal_texts\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     20\u001b[0m )\n",
      "Cell \u001b[0;32mIn[16], line 46\u001b[0m, in \u001b[0;36mSentimentAnalysisFramework.feature_extraction\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     43\u001b[0m test_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto_pandas()\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Preprocessing\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_text)\n\u001b[1;32m     47\u001b[0m test_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m test_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_text)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Bag of Words\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[1;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'text'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Datasets to analyze\n",
    "    datasets = ['imdb', 'yelp_polarity', 'amazon_polarity']\n",
    "    \n",
    "    # Comprehensive results storage\n",
    "    comprehensive_results = {}\n",
    "    \n",
    "    for dataset_name in datasets:\n",
    "        print(f\"\\nAnalyzing {dataset_name} dataset:\")\n",
    "        \n",
    "        # Preprocessing and Feature Extraction\n",
    "        analysis_framework = SentimentAnalysisFramework(dataset_name)\n",
    "        extracted_features = analysis_framework.feature_extraction()\n",
    "        \n",
    "        # Model Comparison\n",
    "        model_comparison = SentimentModelComparison(\n",
    "            extracted_features['features'], \n",
    "            extracted_features['labels'],\n",
    "            extracted_features['original_texts']\n",
    "        )\n",
    "        \n",
    "        # Evaluate Traditional ML Models\n",
    "        traditional_results = model_comparison.traditional_ml_models()\n",
    "        \n",
    "        # # Evaluate Transformer Models\n",
    "        # transformer_results = model_comparison.transformer_models()\n",
    "        \n",
    "        # # Evaluate VADER\n",
    "        # vader_results = model_comparison.vader_sentiment_analysis()\n",
    "        \n",
    "        # # Visualize Results\n",
    "        # model_comparison.visualize_results(traditional_results)\n",
    "        \n",
    "        # Store results\n",
    "        comprehensive_results[dataset_name] = {\n",
    "            'Traditional Models': traditional_results\n",
    "            # 'Transformer Models': transformer_results,\n",
    "            # 'VADER': vader_results\n",
    "        }\n",
    "        \n",
    "        # Print Results\n",
    "        print(\"\\nTraditional ML Models Results:\")\n",
    "        for feature, models in traditional_results.items():\n",
    "            print(f\"\\n{feature} Feature Extraction:\")\n",
    "            for model, metrics in models.items():\n",
    "                print(f\"{model}:\")\n",
    "                for metric, value in metrics.items():\n",
    "                    print(f\"  {metric}: {value:.4f}\")\n",
    "        \n",
    "        # print(\"\\nTransformer Models Results:\")\n",
    "        # for model, metrics in transformer_results.items():\n",
    "        #     print(f\"{model}:\")\n",
    "        #     for metric, value in metrics.items():\n",
    "        #         print(f\"  {metric}: {value:.4f}\")\n",
    "        \n",
    "        # print(\"\\nVADER Results:\")\n",
    "        # for model, metrics in vader_results.items():\n",
    "        #     print(f\"{model}:\")\n",
    "        #     for metric, value in metrics.items():\n",
    "        #             print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing imdb dataset:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'SentimentAnalysisFramework' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAnalyzing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimdb_dataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m dataset:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Preprocessing and Feature Extraction\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m imdb_analysis_framework \u001b[38;5;241m=\u001b[39m SentimentAnalysisFramework(imdb_dataset_name)\n\u001b[1;32m      7\u001b[0m imdb_extracted_features \u001b[38;5;241m=\u001b[39m analysis_framework\u001b[38;5;241m.\u001b[39mfeature_extraction()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SentimentAnalysisFramework' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imdb dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_dataset_name =  'imdb'   #['imdb', 'yelp_polarity', 'amazon_polarity']\n",
    "comprehensive_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing imdb dataset:\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nAnalyzing {imdb_dataset_name} dataset:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing and Feature Extraction\n",
    "imdb_analysis_framework = SentimentAnalysisFramework(imdb_dataset_name)\n",
    "imdb_extracted_features = analysis_framework.feature_extraction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Model Comparison\n",
    "imdb_model_comparison = SentimentModelComparison(\n",
    "            extracted_features['features'], \n",
    "            extracted_features['labels'],\n",
    "            extracted_features['original_texts']\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Skipping Naive Bayes on Word2Vec (contains negative values).\n"
     ]
    }
   ],
   "source": [
    "# Evaluate Traditional ML Models\n",
    "imdb_traditional_results = imdb_model_comparison.traditional_ml_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Bag of Words': {'Logistic Regression': {'Accuracy': 0.84832,\n",
       "   'Precision': 0.848351042815489,\n",
       "   'Recall': 0.84832,\n",
       "   'F1 Score': 0.8483166207370035},\n",
       "  'Decision Tree': {'Accuracy': 0.71088,\n",
       "   'Precision': 0.7108914239040762,\n",
       "   'Recall': 0.71088,\n",
       "   'F1 Score': 0.7108760845682874},\n",
       "  'Naive Bayes': {'Accuracy': 0.83812,\n",
       "   'Precision': 0.8388571123523861,\n",
       "   'Recall': 0.83812,\n",
       "   'F1 Score': 0.8380319181289936}},\n",
       " 'TF-IDF': {'Logistic Regression': {'Accuracy': 0.88004,\n",
       "   'Precision': 0.8800502765594781,\n",
       "   'Recall': 0.88004,\n",
       "   'F1 Score': 0.8800391890649181},\n",
       "  'Decision Tree': {'Accuracy': 0.7112,\n",
       "   'Precision': 0.7113995015994101,\n",
       "   'Recall': 0.7112,\n",
       "   'F1 Score': 0.7111318474523167},\n",
       "  'Naive Bayes': {'Accuracy': 0.84244,\n",
       "   'Precision': 0.8427998575784915,\n",
       "   'Recall': 0.84244,\n",
       "   'F1 Score': 0.842398639098845}},\n",
       " 'Word2Vec': {'Logistic Regression': {'Accuracy': 0.803,\n",
       "   'Precision': 0.8030332822666046,\n",
       "   'Recall': 0.803,\n",
       "   'F1 Score': 0.8029945907042737},\n",
       "  'Decision Tree': {'Accuracy': 0.66636,\n",
       "   'Precision': 0.6663600010647039,\n",
       "   'Recall': 0.66636,\n",
       "   'F1 Score': 0.6663599994661761}}}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_traditional_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/transformers/modeling_utils.py:484: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=map_location)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on SST-2 dataset...\n",
      "  Running philipobiorah/bert-imdb-model...\n",
      "  Running DistilBERT-SST-2...\n",
      "  Running RoBERTa-Sentiment...\n",
      "\n",
      "Evaluating on Yelp dataset...\n",
      "  Running philipobiorah/bert-imdb-model...\n",
      "  Running DistilBERT-SST-2...\n",
      "  Running RoBERTa-Sentiment...\n",
      "\n",
      "Evaluating on Amazon dataset...\n",
      "  Running philipobiorah/bert-imdb-model...\n",
      "  Running DistilBERT-SST-2...\n",
      "  Running RoBERTa-Sentiment...\n",
      "\n",
      "Evaluating on IMDB dataset...\n",
      "  Running philipobiorah/bert-imdb-model...\n",
      "  Running DistilBERT-SST-2...\n",
      "  Running RoBERTa-Sentiment...\n",
      "\n",
      "=== Model Accuracy Comparison ===\n",
      "                        Model  Amazon  IMDB  SST-2  Yelp\n",
      "             DistilBERT-SST-2    0.85  0.89    0.0  0.85\n",
      "            RoBERTa-Sentiment    0.47  0.79    0.0  0.42\n",
      "philipobiorah/bert-imdb-model    0.89  0.96    0.0  0.89\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "# Load models and tokenizers\n",
    "models = {\n",
    "    \"philipobiorah/bert-imdb-model\": (\n",
    "        BertForSequenceClassification.from_pretrained(\"philipobiorah/bert-imdb-model\"),\n",
    "        BertTokenizer.from_pretrained(\"bert-base-uncased\"),\n",
    "    ),\n",
    "    \"DistilBERT-SST-2\": (\n",
    "        AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\"),\n",
    "        AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\"),\n",
    "    ),\n",
    "    \"RoBERTa-Sentiment\": (\n",
    "        AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\"),\n",
    "        AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\"),\n",
    "    )\n",
    "}\n",
    "\n",
    "# Evaluation datasets\n",
    "datasets_info = {\n",
    "    \"SST-2\": (\"glue\", \"sst2\", \"sentence\", \"label\"),\n",
    "    \"Yelp\": (\"yelp_polarity\", None, \"text\", \"label\"),\n",
    "    \"Amazon\": (\"amazon_polarity\", None, \"content\", \"label\"),\n",
    "    \"IMDB\": (\"imdb\", None, \"text\", \"label\")\n",
    "}\n",
    "\n",
    "# Predict function\n",
    "def predict(texts, model, tokenizer):\n",
    "    preds = []\n",
    "    for text in texts:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        pred = torch.argmax(outputs.logits, dim=1).item()\n",
    "        preds.append(pred)\n",
    "    return preds\n",
    "\n",
    "# Evaluate models on datasets\n",
    "results = []\n",
    "for dataset_name, (dataset_key, subset, text_key, label_key) in datasets_info.items():\n",
    "    print(f\"\\nEvaluating on {dataset_name} dataset...\")\n",
    "    dataset = load_dataset(dataset_key, subset)\n",
    "    texts = dataset[\"test\"][text_key][:100]  # small batch for speed\n",
    "    labels = dataset[\"test\"][label_key][:100]\n",
    "\n",
    "    for model_name, (model, tokenizer) in models.items():\n",
    "        print(f\"  Running {model_name}...\")\n",
    "        model.eval()\n",
    "        pred_labels = predict(texts, model, tokenizer)\n",
    "        acc = accuracy_score(labels, pred_labels)\n",
    "        results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Dataset\": dataset_name,\n",
    "            \"Accuracy\": round(acc, 4)\n",
    "        })\n",
    "\n",
    "# Display results in plain text\n",
    "df = pd.DataFrame(results)\n",
    "df_pivot = df.pivot(index=\"Model\", columns=\"Dataset\", values=\"Accuracy\").reset_index()\n",
    "\n",
    "print(\"\\n=== Model Accuracy Comparison ===\")\n",
    "print(df_pivot.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/transformers/modeling_utils.py:484: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=map_location)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on SST-2 dataset...\n",
      "  Running philipobiorah/bert-imdb-model...\n",
      "  Running DistilBERT-SST-2...\n",
      "  Running RoBERTa-Sentiment...\n",
      "\n",
      "Evaluating on Yelp dataset...\n",
      "  Running philipobiorah/bert-imdb-model...\n",
      "  Running DistilBERT-SST-2...\n",
      "  Running RoBERTa-Sentiment...\n",
      "\n",
      "Evaluating on Amazon dataset...\n",
      "  Running philipobiorah/bert-imdb-model...\n",
      "  Running DistilBERT-SST-2...\n",
      "  Running RoBERTa-Sentiment...\n",
      "\n",
      "Evaluating on IMDB dataset...\n",
      "  Running philipobiorah/bert-imdb-model...\n",
      "  Running DistilBERT-SST-2...\n",
      "  Running RoBERTa-Sentiment...\n",
      "\n",
      "=== Model Accuracy Comparison ===\n",
      "                        Model  Amazon  IMDB  SST-2  Yelp\n",
      "             DistilBERT-SST-2    0.85  0.89   0.94  0.85\n",
      "            RoBERTa-Sentiment    0.47  0.79   0.40  0.42\n",
      "philipobiorah/bert-imdb-model    0.89  0.96   0.89  0.89\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "# Load models and tokenizers\n",
    "models = {\n",
    "    \"philipobiorah/bert-imdb-model\": (\n",
    "        BertForSequenceClassification.from_pretrained(\"philipobiorah/bert-imdb-model\"),\n",
    "        BertTokenizer.from_pretrained(\"bert-base-uncased\"),\n",
    "    ),\n",
    "    \"DistilBERT-SST-2\": (\n",
    "        AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\"),\n",
    "        AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\"),\n",
    "    ),\n",
    "    \"RoBERTa-Sentiment\": (\n",
    "        AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\"),\n",
    "        AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\"),\n",
    "    )\n",
    "}\n",
    "\n",
    "# Evaluation datasets\n",
    "datasets_info = {\n",
    "    \"SST-2\": (\"glue\", \"sst2\", \"sentence\", \"label\"),\n",
    "    \"Yelp\": (\"yelp_polarity\", None, \"text\", \"label\"),\n",
    "    \"Amazon\": (\"amazon_polarity\", None, \"content\", \"label\"),\n",
    "    \"IMDB\": (\"imdb\", None, \"text\", \"label\")\n",
    "}\n",
    "\n",
    "# Predict function\n",
    "def predict(texts, model, tokenizer):\n",
    "    preds = []\n",
    "    for text in texts:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        pred = torch.argmax(outputs.logits, dim=1).item()\n",
    "        preds.append(pred)\n",
    "    return preds\n",
    "\n",
    "# Evaluate models on datasets\n",
    "results = []\n",
    "for dataset_name, (dataset_key, subset, text_key, label_key) in datasets_info.items():\n",
    "    print(f\"\\nEvaluating on {dataset_name} dataset...\")\n",
    "    dataset = load_dataset(dataset_key, subset)\n",
    "\n",
    "    # Use 'validation' split for SST-2; others use 'test'\n",
    "    split = \"validation\" if dataset_name == \"SST-2\" else \"test\"\n",
    "    texts = dataset[split][text_key][:100]  # small batch for speed\n",
    "    labels = dataset[split][label_key][:100]\n",
    "\n",
    "    for model_name, (model, tokenizer) in models.items():\n",
    "        print(f\"  Running {model_name}...\")\n",
    "        model.eval()\n",
    "        pred_labels = predict(texts, model, tokenizer)\n",
    "        acc = accuracy_score(labels, pred_labels)\n",
    "        results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Dataset\": dataset_name,\n",
    "            \"Accuracy\": round(acc, 4)\n",
    "        })\n",
    "\n",
    "# Display results in plain text\n",
    "df = pd.DataFrame(results)\n",
    "df_pivot = df.pivot(index=\"Model\", columns=\"Dataset\", values=\"Accuracy\").reset_index()\n",
    "\n",
    "print(\"\\n=== Model Accuracy Comparison ===\")\n",
    "print(df_pivot.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/transformers/modeling_utils.py:484: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=map_location)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on SST-2 dataset...\n",
      "  Running philipobiorah/bert-imdb-model...\n",
      "  Running DistilBERT-SST-2...\n",
      "  Running RoBERTa-Sentiment...\n",
      "\n",
      "Evaluating on Yelp dataset...\n",
      "  Running philipobiorah/bert-imdb-model...\n",
      "  Running DistilBERT-SST-2...\n",
      "  Running RoBERTa-Sentiment...\n",
      "\n",
      "Evaluating on Amazon dataset...\n",
      "  Running philipobiorah/bert-imdb-model...\n",
      "  Running DistilBERT-SST-2...\n",
      "  Running RoBERTa-Sentiment...\n",
      "\n",
      "Evaluating on IMDB dataset...\n",
      "  Running philipobiorah/bert-imdb-model...\n",
      "  Running DistilBERT-SST-2...\n",
      "  Running RoBERTa-Sentiment...\n",
      "\n",
      "=== Model Evaluation Metrics ===\n",
      "                        Model Dataset  Accuracy  Precision  Recall  F1-Score\n",
      "philipobiorah/bert-imdb-model   SST-2      0.89     0.8973    0.89    0.8891\n",
      "             DistilBERT-SST-2   SST-2      0.94     0.9406    0.94    0.9399\n",
      "            RoBERTa-Sentiment   SST-2      0.40     0.6337    0.40    0.4885\n",
      "philipobiorah/bert-imdb-model    Yelp      0.89     0.8983    0.89    0.8898\n",
      "             DistilBERT-SST-2    Yelp      0.85     0.8541    0.85    0.8499\n",
      "            RoBERTa-Sentiment    Yelp      0.42     0.7037    0.42    0.4956\n",
      "philipobiorah/bert-imdb-model  Amazon      0.89     0.8933    0.89    0.8894\n",
      "             DistilBERT-SST-2  Amazon      0.85     0.8518    0.85    0.8501\n",
      "            RoBERTa-Sentiment  Amazon      0.47     0.7823    0.47    0.5414\n",
      "philipobiorah/bert-imdb-model    IMDB      0.96     1.0000    0.96    0.9796\n",
      "             DistilBERT-SST-2    IMDB      0.89     1.0000    0.89    0.9418\n",
      "            RoBERTa-Sentiment    IMDB      0.79     1.0000    0.79    0.8827\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    BertForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "# Load models and tokenizers\n",
    "models = {\n",
    "    \"philipobiorah/bert-imdb-model\": (\n",
    "        BertForSequenceClassification.from_pretrained(\"philipobiorah/bert-imdb-model\"),\n",
    "        BertTokenizer.from_pretrained(\"bert-base-uncased\"),\n",
    "    ),\n",
    "    \"DistilBERT-SST-2\": (\n",
    "        AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\"),\n",
    "        AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\"),\n",
    "    ),\n",
    "    \"RoBERTa-Sentiment\": (\n",
    "        AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\"),\n",
    "        AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\"),\n",
    "    )\n",
    "}\n",
    "\n",
    "# Evaluation datasets\n",
    "datasets_info = {\n",
    "    \"SST-2\": (\"glue\", \"sst2\", \"sentence\", \"label\"),\n",
    "    \"Yelp\": (\"yelp_polarity\", None, \"text\", \"label\"),\n",
    "    \"Amazon\": (\"amazon_polarity\", None, \"content\", \"label\"),\n",
    "    \"IMDB\": (\"imdb\", None, \"text\", \"label\")\n",
    "}\n",
    "\n",
    "# Predict function\n",
    "def predict(texts, model, tokenizer):\n",
    "    preds = []\n",
    "    for text in texts:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        pred = torch.argmax(outputs.logits, dim=1).item()\n",
    "        preds.append(pred)\n",
    "    return preds\n",
    "\n",
    "# Evaluate models on datasets\n",
    "results = []\n",
    "for dataset_name, (dataset_key, subset, text_key, label_key) in datasets_info.items():\n",
    "    print(f\"\\nEvaluating on {dataset_name} dataset...\")\n",
    "    dataset = load_dataset(dataset_key, subset)\n",
    "\n",
    "    # Use 'validation' split for SST-2; others use 'test'\n",
    "    split = \"validation\" if dataset_name == \"SST-2\" else \"test\"\n",
    "    texts = dataset[split][text_key][:100]  # small batch for speed\n",
    "    labels = dataset[split][label_key][:100]\n",
    "\n",
    "    for model_name, (model, tokenizer) in models.items():\n",
    "        print(f\"  Running {model_name}...\")\n",
    "        model.eval()\n",
    "        pred_labels = predict(texts, model, tokenizer)\n",
    "        acc = accuracy_score(labels, pred_labels)\n",
    "        precision = precision_score(labels, pred_labels, average='weighted', zero_division=0)\n",
    "        recall = recall_score(labels, pred_labels, average='weighted', zero_division=0)\n",
    "        f1 = f1_score(labels, pred_labels, average='weighted', zero_division=0)\n",
    "        results.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Dataset\": dataset_name,\n",
    "            \"Accuracy\": round(acc, 4),\n",
    "            \"Precision\": round(precision, 4),\n",
    "            \"Recall\": round(recall, 4),\n",
    "            \"F1-Score\": round(f1, 4)\n",
    "        })\n",
    "\n",
    "# Display results in plain text\n",
    "df = pd.DataFrame(results)\n",
    "print(\"\\n=== Model Evaluation Metrics ===\")\n",
    "print(df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing amazon_polarity dataset:\n",
      "⚠️ Skipping Naive Bayes on Word2Vec (contains negative values).\n"
     ]
    }
   ],
   "source": [
    "amazon_polarity_dataset_name =  'amazon_polarity'   #['imdb', 'yelp_polarity', 'amazon_polarity']\n",
    "amazon_polarity_comprehensive_results = {}\n",
    "print(f\"\\nAnalyzing {amazon_polarity_dataset_name} dataset:\")\n",
    "\n",
    "# Preprocessing and Feature Extraction\n",
    "amazon_polarity_analysis_framework = SentimentAnalysisFramework(amazon_polarity_dataset_name)\n",
    "amazon_polarity_extracted_features = amazon_polarity_analysis_framework.feature_extraction()\n",
    "\n",
    "\n",
    " # Model Comparison\n",
    "amazon_polarity_model_comparison = SentimentModelComparison(\n",
    "            amazon_polarity_extracted_features['features'], \n",
    "            amazon_polarity_extracted_features['labels'],\n",
    "            amazon_polarity_extracted_features['original_texts']\n",
    "        )\n",
    "# Evaluate Traditional ML Models\n",
    "amazon_polarity_traditional_results = amazon_polarity_model_comparison.traditional_ml_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Bag of Words': {'Logistic Regression': {'Accuracy': 0.86613,\n",
       "   'Precision': 0.8662564377289097,\n",
       "   'Recall': 0.86613,\n",
       "   'F1 Score': 0.8661184454729299},\n",
       "  'Decision Tree': {'Accuracy': 0.754625,\n",
       "   'Precision': 0.7546334188174096,\n",
       "   'Recall': 0.754625,\n",
       "   'F1 Score': 0.7546229718055013},\n",
       "  'Naive Bayes': {'Accuracy': 0.819685,\n",
       "   'Precision': 0.8196932501318902,\n",
       "   'Recall': 0.819685,\n",
       "   'F1 Score': 0.8196838366722405}},\n",
       " 'TF-IDF': {'Logistic Regression': {'Accuracy': 0.86661,\n",
       "   'Precision': 0.8666284297145678,\n",
       "   'Recall': 0.86661,\n",
       "   'F1 Score': 0.8666083236634686},\n",
       "  'Decision Tree': {'Accuracy': 0.7478975,\n",
       "   'Precision': 0.7478977864768795,\n",
       "   'Recall': 0.7478975,\n",
       "   'F1 Score': 0.7478974271659911},\n",
       "  'Naive Bayes': {'Accuracy': 0.82035,\n",
       "   'Precision': 0.8203514262365846,\n",
       "   'Recall': 0.82035,\n",
       "   'F1 Score': 0.8203498000448362}},\n",
       " 'Word2Vec': {'Logistic Regression': {'Accuracy': 0.8475075,\n",
       "   'Precision': 0.8475086701398817,\n",
       "   'Recall': 0.8475075,\n",
       "   'F1 Score': 0.8475073716307523},\n",
       "  'Decision Tree': {'Accuracy': 0.72631,\n",
       "   'Precision': 0.7263102354531689,\n",
       "   'Recall': 0.72631,\n",
       "   'F1 Score': 0.7263099288132123}}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_polarity_traditional_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: vaderSentiment in /opt/homebrew/anaconda3/lib/python3.11/site-packages (3.3.2)\n",
      "Requirement already satisfied: requests in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from vaderSentiment) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from requests->vaderSentiment) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from requests->vaderSentiment) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from requests->vaderSentiment) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/anaconda3/lib/python3.11/site-packages (from requests->vaderSentiment) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing imdb dataset:\n",
      "\n",
      "VADER Results:\n",
      "VADER:\n",
      "  Accuracy: 0.6974\n",
      "\n",
      "Analyzing yelp_polarity dataset:\n",
      "\n",
      "VADER Results:\n",
      "VADER:\n",
      "  Accuracy: 0.7134\n",
      "\n",
      "Analyzing amazon_polarity dataset:\n",
      "\n",
      "VADER Results:\n",
      "VADER:\n",
      "  Accuracy: 0.6966\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    # Datasets to analyze\n",
    "datasets = ['imdb', 'yelp_polarity', 'amazon_polarity']\n",
    "    \n",
    "    # Comprehensive results storage\n",
    "comprehensive_results = {}\n",
    "    \n",
    "for dataset_name in datasets:\n",
    "        print(f\"\\nAnalyzing {dataset_name} dataset:\")\n",
    "        \n",
    "        # Preprocessing and Feature Extraction\n",
    "        analysis_framework = SentimentAnalysisFramework(dataset_name)\n",
    "        extracted = analysis_framework.feature_extraction()\n",
    "        \n",
    "        # Model Comparison\n",
    "        model_comparison = SentimentModelComparison(\n",
    "            extracted['features'], \n",
    "            extracted['labels'],\n",
    "            extracted['original_texts']\n",
    "        )\n",
    "        \n",
    "        # # Traditional ML Models\n",
    "        # traditional_results = model_comparison.traditional_ml_models()\n",
    "        \n",
    "        # VADER Sentiment Analysis\n",
    "        vader_results = model_comparison.vader_sentiment_analysis()\n",
    "\n",
    "        # (Optional) Visualize\n",
    "        # model_comparison.visualize_results(traditional_results, {}, vader_results)\n",
    "        \n",
    "        # Store results\n",
    "        comprehensive_results[dataset_name] = {\n",
    "            # 'Traditional Models': traditional_results,\n",
    "            'VADER': vader_results\n",
    "        }\n",
    "        \n",
    "        # # Print Traditional ML results\n",
    "        # print(\"\\nTraditional ML Models Results:\")\n",
    "        # for feature, models in traditional_results.items():\n",
    "        #     print(f\"\\n{feature} Feature Extraction:\")\n",
    "        #     for model, metrics in models.items():\n",
    "        #         print(f\"{model}:\")\n",
    "        #         for metric, value in metrics.items():\n",
    "        #             print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "        # Print VADER results\n",
    "        print(\"\\nVADER Results:\")\n",
    "        for model, metrics in vader_results.items():\n",
    "            print(f\"{model}:\")\n",
    "            for metric, value in metrics.items():\n",
    "                print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'VADER': {'Accuracy': 0.6974}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vader_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
