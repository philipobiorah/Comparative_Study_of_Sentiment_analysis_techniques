{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Assessing imdb dataset:\n",
      "\n",
      "Dataset Overview:\n",
      "Total Train Samples: 25000\n",
      "Total Test Samples: 25000\n",
      "Columns: ['text', 'label']\n",
      "\n",
      "Label Distribution:\n",
      "Train: {0: 0.5, 1: 0.5}\n",
      "Test: {0: 0.5, 1: 0.5}\n",
      "\n",
      "Text Characteristics:\n",
      "Train: {'Avg Text Length': np.float64(1325.06964), 'Median Text Length': np.float64(979.0), 'Max Text Length': np.int64(13704), 'Min Text Length': np.int64(52), 'Avg Word Count': np.float64(233.7872)}\n",
      "Test: {'Avg Text Length': np.float64(1293.7924), 'Median Text Length': np.float64(962.0), 'Max Text Length': np.int64(12988), 'Min Text Length': np.int64(32), 'Avg Word Count': np.float64(228.52668)}\n",
      "\n",
      "Duplicate Analysis:\n",
      "Train Duplicates: 96\n",
      "Test Duplicates: 199\n",
      "Train Duplicate Percentage: 0.384\n",
      "Test Duplicate Percentage: 0.796\n",
      "\n",
      "Missing Values:\n",
      "Train: {'text': 0, 'label': 0}\n",
      "Test: {'text': 0, 'label': 0}\n",
      "\n",
      "Advanced Text Analysis:\n",
      "Train: {'Avg Special Character Ratio': np.float64(0.0403472361543248), 'Avg Unique Words': np.float64(149.46488), 'Max Unique Words': np.int64(822), 'Min Unique Words': np.int64(10)}\n",
      "Test: {'Avg Special Character Ratio': np.float64(0.040333312140476384), 'Avg Unique Words': np.float64(146.63468), 'Max Unique Words': np.int64(1092), 'Min Unique Words': np.int64(4)}\n",
      "\n",
      "Assessing yelp_polarity dataset:\n",
      "\n",
      "Dataset Overview:\n",
      "Total Train Samples: 560000\n",
      "Total Test Samples: 38000\n",
      "Columns: ['text', 'label']\n",
      "\n",
      "Label Distribution:\n",
      "Train: {0: 0.5, 1: 0.5}\n",
      "Test: {1: 0.5, 0: 0.5}\n",
      "\n",
      "Text Characteristics:\n",
      "Train: {'Avg Text Length': np.float64(726.4979232142857), 'Median Text Length': np.float64(528.0), 'Max Text Length': np.int64(5273), 'Min Text Length': np.int64(1), 'Avg Word Count': np.float64(133.0288732142857)}\n",
      "Test: {'Avg Text Length': np.float64(723.8446578947369), 'Median Text Length': np.float64(528.0), 'Max Text Length': np.int64(5107), 'Min Text Length': np.int64(4), 'Avg Word Count': np.float64(132.55863157894737)}\n",
      "\n",
      "Duplicate Analysis:\n",
      "Train Duplicates: 0\n",
      "Test Duplicates: 0\n",
      "Train Duplicate Percentage: 0.0\n",
      "Test Duplicate Percentage: 0.0\n",
      "\n",
      "Missing Values:\n",
      "Train: {'text': 0, 'label': 0}\n",
      "Test: {'text': 0, 'label': 0}\n",
      "\n",
      "Advanced Text Analysis:\n",
      "Train: {'Avg Special Character Ratio': np.float64(0.038206779708189226), 'Avg Unique Words': np.float64(89.22570357142857), 'Max Unique Words': np.int64(528), 'Min Unique Words': np.int64(1)}\n",
      "Test: {'Avg Special Character Ratio': np.float64(0.03803976398563485), 'Avg Unique Words': np.float64(89.00407894736843), 'Max Unique Words': np.int64(507), 'Min Unique Words': np.int64(1)}\n",
      "\n",
      "Assessing amazon_polarity dataset:\n",
      "\n",
      "Dataset Overview:\n",
      "Total Train Samples: 3600000\n",
      "Total Test Samples: 400000\n",
      "Columns: ['label', 'title', 'text']\n",
      "\n",
      "Label Distribution:\n",
      "Train: {1: 0.5, 0: 0.5}\n",
      "Test: {1: 0.5, 0: 0.5}\n",
      "\n",
      "Text Characteristics:\n",
      "Train: {'Avg Text Length': np.float64(405.1396275), 'Median Text Length': np.float64(356.0), 'Max Text Length': np.int64(1010), 'Min Text Length': np.int64(4), 'Avg Word Count': np.float64(74.16884944444445)}\n",
      "Test: {'Avg Text Length': np.float64(404.9001975), 'Median Text Length': np.float64(356.0), 'Max Text Length': np.int64(1009), 'Min Text Length': np.int64(15), 'Avg Word Count': np.float64(74.1066475)}\n",
      "\n",
      "Duplicate Analysis:\n",
      "Train Duplicates: 0\n",
      "Test Duplicates: 0\n",
      "Train Duplicate Percentage: 0.0\n",
      "Test Duplicate Percentage: 0.0\n",
      "\n",
      "Missing Values:\n",
      "Train: {'label': 0, 'title': 0, 'text': 0}\n",
      "Test: {'label': 0, 'title': 0, 'text': 0}\n",
      "\n",
      "Advanced Text Analysis:\n",
      "Train: {'Avg Special Character Ratio': np.float64(0.031232323647869684), 'Avg Unique Words': np.float64(56.136560277777775), 'Max Unique Words': np.int64(148), 'Min Unique Words': np.int64(1)}\n",
      "Test: {'Avg Special Character Ratio': np.float64(0.031232541844300955), 'Avg Unique Words': np.float64(56.112455), 'Max Unique Words': np.int64(141), 'Min Unique Words': np.int64(1)}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "class SentimentDatasetQualityAssessment:\n",
    "    def __init__(self, dataset_name):\n",
    "        try:\n",
    "            # Load dataset using explicit import\n",
    "            self.dataset_name = dataset_name\n",
    "            \n",
    "            # Special handling for different datasets\n",
    "            if dataset_name == 'imdb':\n",
    "                self.dataset = load_dataset(dataset_name, split=['train', 'test', 'unsupervised'])\n",
    "                self.train_df = pd.DataFrame(self.dataset[0])\n",
    "                self.test_df = pd.DataFrame(self.dataset[1])\n",
    "            elif dataset_name == 'amazon_polarity':\n",
    "                # Amazon Polarity dataset specific handling\n",
    "                self.dataset = load_dataset(dataset_name)\n",
    "                self.train_df = self.dataset['train'].to_pandas()\n",
    "                self.test_df = self.dataset['test'].to_pandas()\n",
    "                \n",
    "                # Ensure proper column naming\n",
    "                if len(self.train_df.columns) > 2:\n",
    "                    self.train_df.columns = ['label', 'title', 'text']\n",
    "                    self.test_df.columns = ['label', 'title', 'text']\n",
    "            else:\n",
    "                # Standard dataset loading\n",
    "                self.dataset = load_dataset(dataset_name)\n",
    "                self.train_df = self.dataset['train'].to_pandas()\n",
    "                self.test_df = self.dataset['test'].to_pandas()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading dataset {dataset_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_comprehensive_analysis(self):\n",
    "        \"\"\"\n",
    "        Generate a comprehensive analysis report\n",
    "        \"\"\"\n",
    "        # Flexible text column detection\n",
    "        text_columns = ['text', 'Text', 'content', 'review']\n",
    "        text_column = next((col for col in text_columns if col in self.train_df.columns), \n",
    "                           self.train_df.columns[1] if len(self.train_df.columns) > 1 else self.train_df.columns[0])\n",
    "        \n",
    "        # Flexible label column detection\n",
    "        label_columns = ['label', 'Label', 'sentiment', 'Sentiment']\n",
    "        label_column = next((col for col in label_columns if col in self.train_df.columns), \n",
    "                            self.train_df.columns[0])\n",
    "        \n",
    "        # Detailed Analysis\n",
    "        analysis = {\n",
    "            'Dataset Overview': {\n",
    "                'Total Train Samples': len(self.train_df),\n",
    "                'Total Test Samples': len(self.test_df),\n",
    "                'Columns': list(self.train_df.columns)\n",
    "            },\n",
    "            \n",
    "            'Label Distribution': {\n",
    "                'Train': self.train_df[label_column].value_counts(normalize=True).to_dict(),\n",
    "                'Test': self.test_df[label_column].value_counts(normalize=True).to_dict()\n",
    "            },\n",
    "            \n",
    "            'Text Characteristics': {\n",
    "                'Train': {\n",
    "                    'Avg Text Length': self.train_df[text_column].str.len().mean(),\n",
    "                    'Median Text Length': self.train_df[text_column].str.len().median(),\n",
    "                    'Max Text Length': self.train_df[text_column].str.len().max(),\n",
    "                    'Min Text Length': self.train_df[text_column].str.len().min(),\n",
    "                    'Avg Word Count': self.train_df[text_column].str.split().str.len().mean()\n",
    "                },\n",
    "                'Test': {\n",
    "                    'Avg Text Length': self.test_df[text_column].str.len().mean(),\n",
    "                    'Median Text Length': self.test_df[text_column].str.len().median(),\n",
    "                    'Max Text Length': self.test_df[text_column].str.len().max(),\n",
    "                    'Min Text Length': self.test_df[text_column].str.len().min(),\n",
    "                    'Avg Word Count': self.test_df[text_column].str.split().str.len().mean()\n",
    "                }\n",
    "            },\n",
    "            \n",
    "            'Duplicate Analysis': {\n",
    "                'Train Duplicates': self.train_df.duplicated().sum(),\n",
    "                'Test Duplicates': self.test_df.duplicated().sum(),\n",
    "                'Train Duplicate Percentage': self.train_df.duplicated().mean() * 100,\n",
    "                'Test Duplicate Percentage': self.test_df.duplicated().mean() * 100\n",
    "            },\n",
    "            \n",
    "            'Missing Values': {\n",
    "                'Train': self.train_df.isnull().sum().to_dict(),\n",
    "                'Test': self.test_df.isnull().sum().to_dict()\n",
    "            },\n",
    "            \n",
    "            'Advanced Text Analysis': self._advanced_text_analysis(text_column)\n",
    "        }\n",
    "        \n",
    "        return analysis\n",
    "\n",
    "    def _advanced_text_analysis(self, text_column):\n",
    "        \"\"\"\n",
    "        Perform advanced text analysis\n",
    "        \"\"\"\n",
    "        def text_complexity_metrics(texts):\n",
    "            # Convert to string and lowercase\n",
    "            texts = texts.astype(str).str.lower()\n",
    "            \n",
    "            # Special character analysis\n",
    "            special_char_ratio = texts.apply(lambda x: len(re.findall(r'[^a-z0-9\\s]', x)) / len(x) if len(x) > 0 else 0)\n",
    "            \n",
    "            # Unique word analysis\n",
    "            unique_words = texts.apply(lambda x: len(set(x.split())))\n",
    "            \n",
    "            return {\n",
    "                'Avg Special Character Ratio': special_char_ratio.mean(),\n",
    "                'Avg Unique Words': unique_words.mean(),\n",
    "                'Max Unique Words': unique_words.max(),\n",
    "                'Min Unique Words': unique_words.min()\n",
    "            }\n",
    "        \n",
    "        return {\n",
    "            'Train': text_complexity_metrics(self.train_df[text_column]),\n",
    "            'Test': text_complexity_metrics(self.test_df[text_column])\n",
    "        }\n",
    "\n",
    "    def visualize_analysis(self, analysis):\n",
    "        \"\"\"\n",
    "        Create visualizations for the analysis\n",
    "        \"\"\"\n",
    "        # Label Distribution\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        train_labels = pd.Series(analysis['Label Distribution']['Train'])\n",
    "        train_labels.plot(kind='pie', autopct='%1.1f%%')\n",
    "        plt.title('Train Label Distribution')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        test_labels = pd.Series(analysis['Label Distribution']['Test'])\n",
    "        test_labels.plot(kind='pie', autopct='%1.1f%%')\n",
    "        plt.title('Test Label Distribution')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{self.dataset_name}_label_distribution.png')\n",
    "        plt.close()\n",
    "        \n",
    "        # Text Length Distribution\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.hist(self.train_df['text'].str.len(), bins=50)\n",
    "        plt.title('Train Text Length Distribution')\n",
    "        plt.xlabel('Text Length')\n",
    "        plt.ylabel('Frequency')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.hist(self.test_df['text'].str.len(), bins=50)\n",
    "        plt.title('Test Text Length Distribution')\n",
    "        plt.xlabel('Text Length')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{self.dataset_name}_text_length_distribution.png')\n",
    "        plt.close()\n",
    "\n",
    "def main():\n",
    "    # List of datasets to assess\n",
    "    datasets = ['imdb', 'yelp_polarity', 'amazon_polarity']\n",
    "    \n",
    "    for dataset_name in datasets:\n",
    "        try:\n",
    "            print(f\"\\nAssessing {dataset_name} dataset:\")\n",
    "            quality_assessor = SentimentDatasetQualityAssessment(dataset_name)\n",
    "            \n",
    "            # Generate comprehensive analysis\n",
    "            analysis = quality_assessor.generate_comprehensive_analysis()\n",
    "            \n",
    "            # Print detailed analysis\n",
    "            for section, details in analysis.items():\n",
    "                print(f\"\\n{section}:\")\n",
    "                if isinstance(details, dict):\n",
    "                    for key, value in details.items():\n",
    "                        print(f\"{key}: {value}\")\n",
    "                else:\n",
    "                    print(details)\n",
    "            \n",
    "            # # Generate visualizations\n",
    "            # quality_assessor.visualize_analysis(analysis)\n",
    "            \n",
    "            # print(f\"\\nVisualizations saved for {dataset_name} dataset\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {dataset_name}: {e}\")\n",
    "\n",
    "# Ensure this is run only if the script is executed directly\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
